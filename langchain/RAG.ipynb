{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0onRt7q1mFss",
        "outputId": "6a6aefb4-58d1-4a43-b519-c1cde76bb1b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pypdf\n",
            "  Using cached pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "Installing collected packages: pypdf\n",
            "Successfully installed pypdf-4.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf\n",
        "!pip install langchain\n",
        "!pip install google-generativeai\n",
        "!pip install langchain_google_genai\n",
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnMUcKI5mKk1"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"C:\\Users\\seeth\\Desktop\\python class\\internship-2024\\langchain\\paper.pdf\")\n",
        "pages = loader.load_and_split()\n",
        "pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nD1yOCrEL_yr"
      },
      "outputs": [],
      "source": [
        "page = \"\".join([p.page_content for p in pages])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9hAjUy5mqj7"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "f = open(\"C:\\Users\\seeth\\Desktop\\python class\\internship-2024\\langchain\\api_key.txt\")\n",
        "key = f.read()\n",
        "\n",
        "genai.configure(api_key=key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "pDFMe0ADm6-c",
        "outputId": "5fb73c85-b674-4d4c-84f7-0b515e406fa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ],
      "source": [
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFcYWMoaxpZx",
        "outputId": "ddb68dc8-9e02-4557-9b9f-a4935486431d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 568, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 506, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 633, which is longer than the specified 500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "110\n",
            "<class 'langchain_core.documents.base.Document'>\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import NLTKTextSplitter\n",
        "\n",
        "text_splitter = NLTKTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "\n",
        "chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "print(len(chunks))\n",
        "\n",
        "print(type(chunks[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8Np_qz00Get"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6j1lo0U_1ORr"
      },
      "outputs": [],
      "source": [
        "embedding_model= GoogleGenerativeAIEmbeddings(google_api_key=key, model=\"models/embedding-001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9EROtNk3VbV"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "db = Chroma.from_documents(chunks, embedding_model, persist_directory=\"./chroma_db_\")\n",
        "db.persist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFlytyRH5Eql"
      },
      "outputs": [],
      "source": [
        "db_connection = Chroma(persist_directory=\"./chroma_db_\", embedding_function=embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNQP8Fp-6gYu",
        "outputId": "d5e3f6e9-c05a-446f-ff8c-eae163ea1212"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'langchain_core.vectorstores.VectorStoreRetriever'>\n"
          ]
        }
      ],
      "source": [
        "retriever = db_connection.as_retriever(search_kwargs={\"k\":5})\n",
        "print(type(retriever))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DP7P1PETHD2P",
        "outputId": "f4e74580-24ff-4149-c6fd-73df714a38cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatSession(\n",
              "    model=genai.GenerativeModel(\n",
              "        model_name='models/gemini-1.5-pro-latest',\n",
              "        generation_config={},\n",
              "        safety_settings={},\n",
              "        tools=None,\n",
              "        system_instruction=None,\n",
              "    ),\n",
              "    history=[]\n",
              ")"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
        "\n",
        "chat = model.start_chat(history=[])\n",
        "\n",
        "chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "bwRAqlCZHIuW",
        "outputId": "38b8e485-4842-4cfe-bad6-afd20a1c9579"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Leave No Context Behind: Spotlight on Infini-attention\n",
            "\n",
            "Analyzing the provided abstract and introduction, the research paper \"Leave No Context Behind\" centers on **Infini-attention**, a new attention mechanism designed to equip Transformer-based Large Language Models (LLMs) with the ability to handle infinitely long inputs while maintaining efficiency in memory usage and computation. \n",
            "\n",
            "**Why Infini-attention takes the lead:**\n",
            "\n",
            "* **Tackling LLM Limitations:** Traditional LLMs face constraints due to the quadratic complexity of attention mechanisms, limiting their context window. Infini-attention introduces a compressive memory component that enables efficient processing of extended sequences without excessive memory demands.\n",
            "* **Merging Local and Global Contexts:**  The paper underscores the significance of both local and global contexts in language comprehension. Infini-attention ingeniously combines masked local attention with long-term linear attention within a single Transformer block, effectively capturing both detailed and long-range dependencies.\n",
            "* **Efficiency and Scalability:**  The research demonstrates the efficiency of Infini-attention, achieving remarkable compression ratios compared to established models like Memorizing Transformers. This efficiency empowers LLMs to scale to infinitely long contexts while maintaining manageable computational costs.\n",
            "* **Seamless Integration:**  Infini-attention is built for smooth integration into existing LLMs using continual pre-training and fine-tuning techniques. The paper showcases successful applications of this approach in tasks such as passkey retrieval and book summarization, underscoring its adaptability and effectiveness. \n",
            "\n",
            "**Supporting Evidence:**\n",
            "\n",
            "* The paper's title, \"Leave No Context Behind,\" directly reflects the emphasis on context and the aim to overcome limitations in processing long sequences.\n",
            "* The abstract highlights Infini-attention as the core element of the proposed methodology.\n",
            "* The introduction and methodology sections dedicate considerable attention to explaining the Infini-attention mechanism and its advantages.\n",
            "* The experimental results consistently showcase the effectiveness of Infini-attention across diverse tasks and benchmarks.\n",
            "\n",
            "**In conclusion,** while the research likely delves into other important facets of contextual language modeling, the development and implementation of Infini-attention clearly emerge as the central and most significant contribution of \"Leave No Context Behind.\" \n",
            "\n"
          ]
        }
      ],
      "source": [
        "user_input = page+ \" previous text is the content of the research paper Leave No Context Behind .based on that answer the question\" + \"What is the most important topic that covered in the research paper named Leave No Context Behind\"\n",
        "\n",
        "response = chat.send_message(user_input)\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwN6lu0xIoJi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
